{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d93b0e-5279-4cde-abd5-d9c69c5da8da",
   "metadata": {},
   "source": [
    "## Import libraries and file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f091cdf8-f91d-49f8-b727-fa59e81c5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from path_location import folder_location\n",
    "\n",
    "ABNORMAL_RETURNS_FOLDER = folder_location.ABNORMAL_RETURNS_FOLDER\n",
    "PROCESSED_DATA_FOLDER = folder_location.PROCESSED_DATA_FOLDER\n",
    "NETWORK_RAW_FOLDERS = folder_location.PROFILE_DATA_FOLDERS\n",
    "TRANSACTIONS_LABELLED_FILE = folder_location.TRANSACTIONS_LABELLED_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc30b5-168b-48b0-989d-52f8486cd263",
   "metadata": {},
   "source": [
    "## Importing relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3265df-5b92-447e-b40c-a7499b521233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SEC_RPTOWNERCIK', 'NODEID'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_name_match = pd.read_csv(f\"{PROCESSED_DATA_FOLDER}/final_final_name_match.csv\")\n",
    "print(df_name_match.columns)\n",
    "# Create a dictionary that maps SEC_RPTOWNERCIK to NODEID\n",
    "mapping_dict = df_name_match.set_index(\"SEC_RPTOWNERCIK\")[\"NODEID\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427db6c2-1807-489c-bd33-8195f3680684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the updated DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRANS_SK</th>\n",
       "      <th>ACCESSION_NUMBER</th>\n",
       "      <th>TRANS_DATE</th>\n",
       "      <th>ISSUERTRADINGSYMBOL</th>\n",
       "      <th>RPTOWNERCIK</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3794004</td>\n",
       "      <td>0001181431-09-023155</td>\n",
       "      <td>2009-05-04</td>\n",
       "      <td>EXAC</td>\n",
       "      <td>1000032</td>\n",
       "      <td>249380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2834113</td>\n",
       "      <td>0001181431-12-005367</td>\n",
       "      <td>2012-01-31</td>\n",
       "      <td>EXAC</td>\n",
       "      <td>1000032</td>\n",
       "      <td>249380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3043733</td>\n",
       "      <td>0001181431-12-047732</td>\n",
       "      <td>2012-08-03</td>\n",
       "      <td>EXAC</td>\n",
       "      <td>1000032</td>\n",
       "      <td>249380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3043734</td>\n",
       "      <td>0001181431-12-047732</td>\n",
       "      <td>2012-08-07</td>\n",
       "      <td>EXAC</td>\n",
       "      <td>1000032</td>\n",
       "      <td>249380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3043284</td>\n",
       "      <td>0001181431-12-049839</td>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>EXAC</td>\n",
       "      <td>1000032</td>\n",
       "      <td>249380.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TRANS_SK      ACCESSION_NUMBER TRANS_DATE ISSUERTRADINGSYMBOL RPTOWNERCIK  \\\n",
       "0   3794004  0001181431-09-023155 2009-05-04                EXAC     1000032   \n",
       "1   2834113  0001181431-12-005367 2012-01-31                EXAC     1000032   \n",
       "2   3043733  0001181431-12-047732 2012-08-03                EXAC     1000032   \n",
       "3   3043734  0001181431-12-047732 2012-08-07                EXAC     1000032   \n",
       "4   3043284  0001181431-12-049839 2012-09-07                EXAC     1000032   \n",
       "\n",
       "         id  \n",
       "0  249380.0  \n",
       "1  249380.0  \n",
       "2  249380.0  \n",
       "3  249380.0  \n",
       "4  249380.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_txns = pd.read_csv(f\"{PROCESSED_DATA_FOLDER}/{TRANSACTIONS_LABELLED_FILE}\",\n",
    "                      usecols=[\"TRANS_SK\", \"ACCESSION_NUMBER\", \"TRANS_DATE\", \"RPTOWNERCIK_;\", \"ISSUERTRADINGSYMBOL\"],\n",
    "                      parse_dates=[\"TRANS_DATE\"])\n",
    "df_txns[\"id\"] = df_txns[\"RPTOWNERCIK_;\"].map(mapping_dict)\n",
    "\n",
    "print(df_txns.columns)\n",
    "print(\"First 5 rows of the updated DataFrame:\")\n",
    "display(df_txns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9672db27-4d8c-48b4-bcc2-04eeaed2d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3171001, 6)\n",
      "Updated 'txns_for_features.csv' with new 'id' column.\n"
     ]
    }
   ],
   "source": [
    "print(df_txns.shape)\n",
    "# Optionally, save the updated DataFrame.\n",
    "df_txns.to_csv(\"txns_for_features.csv\", index=False)\n",
    "print(\"Updated 'txns_for_features.csv' with new 'id' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d070d-5214-4516-94ac-d7fab6c1b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to mapping_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "# We save the dictionary to pickle in case we need it.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/mapping_CIK2Node_txn_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mapping_dict, f)\n",
    "\n",
    "print(\"Dictionary saved to mapping_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fda8df-9259-4c16-9f7c-10b2ee999c69",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## In this notebook, we create dictionaries that parse congress committee membership by date ranges, and in turn split them by committee.\n",
    "\n",
    "### We also construct a dictionary for mapping congressmen to their nodeids for later use. (from ```\"congress_matches_7apr_603pm.csv\"```)\n",
    "\n",
    "1) We import ```\"congress_matches_7apr_603pm.csv\"``` to generate a ```congress_nodeid_mapper``` dictionary.\n",
    "2) We import ```\"house.csv\"``` to generate our dictionary, ```congress_date_subcomm_mapper``` dictionary.\n",
    "3) We import ```\"TIC to SIC.csv\"``` to generate a dictionary that stores the relevant subcommittees for each company.\n",
    "4) We export all three to pickle files.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ba003-292c-4f4b-a935-c58996dfbc80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Step 1: Load Your Data**\n",
    "\n",
    "- **house.csv** contains the following columns (among others):\n",
    "  - **\"ID #\"** – The congress member’s internal ID.\n",
    "  - **\"Name\"** – The member's name.\n",
    "  \n",
    "- **congress_matches_7apr_603pm.csv** contains:\n",
    "  - **\"name\"** – The member's name.\n",
    "  - **\"id\"** – The Littlesis nodeid.\n",
    "  \n",
    "**Step 2: Merge on Name**\n",
    "\n",
    "We merge the two DataFrames on the name fields (assuming the naming is consistent) so that each row now has both the internal congress ID (`\"ID #\"` from house.csv) and the Littlesis nodeid (`\"id\"` from congress_matches file).\n",
    "\n",
    "**Step 3: Build the Dictionary**\n",
    "\n",
    "After merging, we select the columns `\"ID #\"` and `\"id\"`, remove duplicates, and create a dictionary mapping `\"ID #\"` to `\"id\"`. For example, if after the merge we have:\n",
    "\n",
    "| **ID #** | **id** |\n",
    "|----------|--------|\n",
    "| 101      | 5001   |\n",
    "| 102      | 5002   |\n",
    "| 103      | 5003   |\n",
    "\n",
    "Then the resulting dictionary will be:\n",
    "\n",
    "> **congress_mapper[101] = 5001**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d9e94-703e-4e2b-b0d5-2431d327d9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually updated NODEID for Green, Mark (ID #: 29941.0) with NODEID ['13706']\n",
      "Manually updated NODEID for McGovern, Jim (ID #: 29729.0) with NODEID ['75681']\n",
      "Manually updated NODEID for Keating, William (ID #: 21140.0) with NODEID ['91599']\n",
      "Manually updated NODEID for Reed, Thomas W., II (ID #: 21101.0) with NODEID ['96165']\n",
      "Manually updated NODEID for Grisham, Michelle Lujan (ID #: 21341.0) with NODEID ['127277']\n",
      "Manually updated NODEID for Ashford, Brad (ID #: 21533.0) with NODEID ['68986']\n",
      "Manually updated NODEID for Johnson, James M. (Mike) (ID #: 21727.0) with NODEID ['284895']\n",
      "Manually updated NODEID for Cox, TJ (ID #: 21909.0) with NODEID ['284848']\n",
      "Manually updated NODEID for Green, Mark (ID #: 21926.0) with NODEID ['359084']\n",
      "Manually updated NODEID for Nelson, Clarence William (Bill) (ID #: 14651.0) with NODEID ['13497']\n",
      "Manually updated NODEID for Nelson, Earl Benjamin (Ben) (ID #: 40103.0) with NODEID ['13498']\n",
      "Exported final mapping to 'all_matched_bioguide.csv' and 'congress_nodeid_mapper.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD AND MERGE CONGRESS MEMBER DATA\n",
    "# =============================================================================\n",
    "# Load house and senate data (for Congress ≥ 109), keep only \"Name\" and \"ID #\",\n",
    "# drop duplicates, and combine them into a single DataFrame.\n",
    "df_house = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/house.csv\")\n",
    "df_house = df_house[df_house[\"Congress\"] >= 109]\n",
    "df_senate = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/senate_ass.csv\")\n",
    "df_senate = df_senate[df_senate[\"Congress\"] >= 109]\n",
    "\n",
    "house_unique  = df_house[[\"Name\", \"ID #\"]].drop_duplicates()\n",
    "senate_unique = df_senate[[\"Name\", \"ID #\"]].drop_duplicates()\n",
    "unique_members = pd.concat([house_unique, senate_unique], ignore_index=True)\\\n",
    "                   .drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: AUTOMATIC BIOGUIDE MATCHING\n",
    "# =============================================================================\n",
    "def match_bioguide_ids(unique_members: pd.DataFrame, mapper_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Maps member names to bioguide IDs using multiple matching strategies:\n",
    "      1) Full‑name exact match\n",
    "      2) \"Surname, First\" prefix match (unambiguous)\n",
    "      3) Surname‑only match (if one candidate exists)\n",
    "      4) \"Surname, First3\" short‑prefix match (if unambiguous)\n",
    "    Returns a DataFrame with a new column \"bioguide\".\n",
    "    \"\"\"\n",
    "    # Parse the mapper file (each line: \"Lastname, Firstname (Party - State)  BIOMAPID\")\n",
    "    name_to_biog = {}\n",
    "    with open(mapper_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            name_party, bioguide = line.rsplit(maxsplit=1)\n",
    "            name = name_party.split(\" (\")[0]\n",
    "            name_to_biog[name] = bioguide\n",
    "\n",
    "    df = unique_members.copy()\n",
    "    # 2a. Full‑name exact match.\n",
    "    df[\"bioguide\"] = df[\"Name\"].map(name_to_biog)\n",
    "\n",
    "    # 2b. \"Surname, First\" prefix match.\n",
    "    prefix_to_full = {}\n",
    "    for full_name in name_to_biog:\n",
    "        if \", \" in full_name:\n",
    "            surname, rest = full_name.split(\", \", 1)\n",
    "            first = rest.split()[0]\n",
    "            prefix = f\"{surname}, {first}\"\n",
    "            prefix_to_full.setdefault(prefix, []).append(full_name)\n",
    "    prefix_to_biog = {p: name_to_biog[names[0]] \n",
    "                      for p, names in prefix_to_full.items() \n",
    "                      if len(names) == 1}\n",
    "    for idx, row in df[df[\"bioguide\"].isnull()].iterrows():\n",
    "        parts = row[\"Name\"].split(\" (\")[0].split(\", \", 1)\n",
    "        if len(parts) == 2:\n",
    "            surname, rest = parts\n",
    "            first = rest.split()[0]\n",
    "            pref = f\"{surname}, {first}\"\n",
    "            if pref in prefix_to_biog:\n",
    "                df.at[idx, \"bioguide\"] = prefix_to_biog[pref]\n",
    "\n",
    "    # 2c. Surname‑only match.\n",
    "    surname_to_full = {}\n",
    "    for full_name in name_to_biog:\n",
    "        surname = full_name.split(\",\", 1)[0]\n",
    "        surname_to_full.setdefault(surname, []).append(full_name)\n",
    "    for idx, row in df[df[\"bioguide\"].isnull()].iterrows():\n",
    "        surname = row[\"Name\"].split(\",\", 1)[0]\n",
    "        candidates = surname_to_full.get(surname, [])\n",
    "        if len(candidates) == 1:\n",
    "            df.at[idx, \"bioguide\"] = name_to_biog[candidates[0]]\n",
    "\n",
    "    # 2d. \"Surname, First3\" short‑prefix match.\n",
    "    short_to_full = {}\n",
    "    for full_name in name_to_biog:\n",
    "        if \", \" in full_name:\n",
    "            surname, rest = full_name.split(\", \",1)\n",
    "            first = rest.split()[0]\n",
    "            sp = f\"{surname}, {first[:3]}\"\n",
    "            short_to_full.setdefault(sp, []).append(full_name)\n",
    "    short_to_biog = {sp: name_to_biog[names[0]] \n",
    "                     for sp, names in short_to_full.items() \n",
    "                     if len(names) == 1}\n",
    "    for idx, row in df[df[\"bioguide\"].isnull()].iterrows():\n",
    "        parts = row[\"Name\"].split(\" (\")[0].split(\", \", 1)\n",
    "        if len(parts) == 2:\n",
    "            surname, rest = parts\n",
    "            first = rest.split()[0]\n",
    "            sp = f\"{surname}, {first[:3]}\"\n",
    "            if sp in short_to_biog:\n",
    "                df.at[idx, \"bioguide\"] = short_to_biog[sp]\n",
    "    return df\n",
    "\n",
    "df_auto = match_bioguide_ids(unique_members, f\"{NETWORK_RAW_FOLDERS}/name_bioguide_mapper.txt\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: MANUAL CORRECTIONS\n",
    "# =============================================================================\n",
    "# Load manual corrections from manual_fill.csv and merge them with automatic results.\n",
    "df_manual = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/manual_fill.csv\", dtype={\"ID #\": str})[[\"Name\", \"ID #\", \"bioguide\"]]\n",
    "df_auto[\"ID #\"] = df_auto[\"ID #\"].astype(str)\n",
    "df_manual[\"ID #\"] = df_manual[\"ID #\"].astype(str)\n",
    "df_merged = pd.merge(df_auto, df_manual, on=[\"Name\"], how=\"left\", suffixes=(\"\", \"_manual\"))\n",
    "df_merged[\"bioguide\"] = df_merged[\"bioguide\"].combine_first(df_merged[\"bioguide_manual\"])\n",
    "df_merged.drop(columns=[\"bioguide_manual\"], inplace=True)\n",
    "\n",
    "# Build mapping_dict: key = \"ID #\", value = dict with { \"Name\", \"ID #\", \"bioguide\" }.\n",
    "mapping_dict = {\n",
    "    id_num: {\"Name\": row[\"Name\"], \"ID #\": id_num, \"bioguide\": row[\"bioguide\"]}\n",
    "    for id_num, row in df_merged.set_index(\"ID #\").iterrows()\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: UPDATE WITH LITTLESIS NODEIDs\n",
    "# =============================================================================\n",
    "# Load Littlesis entities and create a mapping: bioguide -> list of node IDs.\n",
    "df_entities = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/entities_merged.csv\", dtype=str)\n",
    "df_entities = df_entities.dropna(subset=[\"ext_ElectedRepresentative_bioguide_id\"])\n",
    "df_entities[\"ext_ElectedRepresentative_bioguide_id\"] = df_entities[\"ext_ElectedRepresentative_bioguide_id\"].astype(str)\n",
    "df_entities[\"id\"] = df_entities[\"id\"].astype(str)\n",
    "littlesis_map = df_entities.groupby(\"ext_ElectedRepresentative_bioguide_id\")[\"id\"].apply(list).to_dict()\n",
    "\n",
    "# For each member in mapping_dict, update with NODEID (list of node ids) if available.\n",
    "for key, entry in mapping_dict.items():\n",
    "    bg = entry.get(\"bioguide\")\n",
    "    if bg and bg in littlesis_map:\n",
    "        mapping_dict[key][\"NODEID\"] = littlesis_map[bg]\n",
    "    else:\n",
    "        mapping_dict[key][\"NODEID\"] = []\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: APPLY MANUAL UPDATES TO THE MAPPING DICTIONARY\n",
    "# =============================================================================\n",
    "manual_updates = {\n",
    "    \"29941.0\": {\"bioguide\": \"G000545\", \"NODEID\": [\"13706\"]},\n",
    "    \"29729.0\": {\"bioguide\": \"M000452\", \"NODEID\": [\"75681\"]},\n",
    "    \"21140.0\": {\"bioguide\": \"K000037\", \"NODEID\": [\"91599\"]},\n",
    "    \"21101.0\": {\"bioguide\": \"R000134\", \"NODEID\": [\"96165\"]},\n",
    "    \"21341.0\": {\"bioguide\": \"G000480\", \"NODEID\": [\"127277\"]},\n",
    "    \"21533.0\": {\"bioguide\": \"A000373\", \"NODEID\": [\"68986\"]},\n",
    "    \"21727.0\": {\"bioguide\": \"J000148\", \"NODEID\": [\"284895\"]},\n",
    "    \"21909.0\": {\"bioguide\": \"C001124\", \"NODEID\": [\"284848\"]},\n",
    "    \"21926.0\": {\"bioguide\": \"G000545\", \"NODEID\": [\"359084\"]},\n",
    "    \"14651.0\": {\"bioguide\": \"N000146\", \"NODEID\": [\"13497\"]},\n",
    "    \"40103.0\": {\"bioguide\": \"N000073\", \"NODEID\": [\"13498\"]}\n",
    "}\n",
    "\n",
    "# Iterate over the manual_updates and update mapping_dict accordingly.\n",
    "for member_id, manual_info in manual_updates.items():\n",
    "    updated = False\n",
    "    for key, entry in mapping_dict.items():\n",
    "        if str(entry.get(\"ID #\")).strip() == member_id:\n",
    "            if entry.get(\"bioguide\") == manual_info.get(\"bioguide\") or not entry.get(\"bioguide\"):\n",
    "                mapping_dict[key][\"NODEID\"] = manual_info.get(\"NODEID\")\n",
    "                print(f\"Manually updated NODEID for {entry.get('Name')} (ID #: {member_id}) with NODEID {manual_info.get('NODEID')}\")\n",
    "                updated = True\n",
    "    if not updated:\n",
    "        print(f\"No mapping_dict entry found with ID # {member_id} for manual update.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: EXPORT FINAL MAPPING\n",
    "# =============================================================================\n",
    "# Optionally build a DataFrame for CSV output.\n",
    "df_mapping = pd.DataFrame(list(mapping_dict.values()))\n",
    "df_mapping.set_index(\"ID #\", inplace=True)\n",
    "df_mapping.reset_index(inplace=True)\n",
    "#df_mapping.to_csv(\"all_matched_bioguide.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Export mapping_dict to a pickle file.\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/congress_nodeid_mapper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mapping_dict, f)\n",
    "    \n",
    "print(\"Exported final mapping to 'all_matched_bioguide.csv' and 'congress_nodeid_mapper.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b79a89-20ba-497f-a7f5-687778b10706",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Building the `congress_date_subcomm_mapper`**\n",
    "\n",
    "Next, we process **`house.csv`**. This file has a row per congress member with their start (assignment) and end (termination) dates, plus a subcommittee name (in the column **\"Committee Name\"**).\n",
    "\n",
    "### **Steps:**\n",
    "\n",
    "1. **Preprocess `house.csv`:**  \n",
    "   Convert the `\"Date of Assignment\"` and `\"Date of Termination\"` columns into datetime objects.\n",
    "\n",
    "2. **For each subcommittee:**  \n",
    "   - **Extract rows for that subcommittee.**  \n",
    "   - Create two events per row:  \n",
    "     - A **join** event on the assignment date.  \n",
    "     - A **leave** event on the termination date **+ 1 day** (so the member is active through their final day).  \n",
    "     \n",
    "   **Example Events:**\n",
    "\n",
    "   | **ID #** | **Event Date** | **Event** | **Subcommittee**   |\n",
    "   |----------|----------------|-----------|--------------------|\n",
    "   | 1        | 2020‑01‑01     | join      | *Subcommittee A*   |\n",
    "   | 1        | 2020‑01‑04     | leave     | *Subcommittee A*   |\n",
    "   | 2        | 2020‑01‑02     | join      | *Subcommittee A*   |\n",
    "   | 2        | 2020‑01‑03     | leave     | *Subcommittee A*   |\n",
    "\n",
    "\n",
    "3. **Sort events and “sweep” through them:**  \n",
    "   As you iterate through the sorted events, maintain a set of active members. After processing each event, record a snapshot (as a sorted list) in a dictionary keyed by the event date.\n",
    "\n",
    "4. **Store snapshots in a nested dictionary:**  \n",
    "   The final **`congress_date_subcomm_mapper`** will map each subcommittee name to its membership timeline.  \n",
    "\n",
    "   **Example:**\n",
    "\n",
    "   ```python\n",
    "   congress_date_subcomm_mapper = {\n",
    "       \"Subcommittee A\": {\n",
    "           Timestamp(\"2020-01-01 00:00:00\"): [1],\n",
    "           Timestamp(\"2020-01-02 00:00:00\"): [1, 2],\n",
    "           Timestamp(\"2020-01-03 00:00:00\"): [1],    # After member 2 left.\n",
    "           Timestamp(\"2020-01-04 00:00:00\"): []        # After member 1 left.\n",
    "       },\n",
    "       \"Subcommittee B\": {\n",
    "           ... # Similar structure for another subcommittee.\n",
    "       }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fc19c-381c-48d4-9c0c-ca0a92961dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Congress', 'Committee code', 'ID #', 'Name', 'Maj/Min',\n",
      "       'Rank Within Party Status', 'Party', 'Date of Assignment',\n",
      "       'Date of Termination', 'Senior Party Member', 'Committee Seniority',\n",
      "       'Committee Period of Service',\n",
      "       'Committee status at end of this Congress',\n",
      "       'Committee continuity of assignment in next Congress',\n",
      "       'Appointment Citation', 'Committee Name', 'State', 'CD', 'State Name',\n",
      "       'Notes', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23'],\n",
      "      dtype='object')\n",
      "Index(['Congress', 'Committee Code', 'ID #', 'Name', 'Maj/Min',\n",
      "       'Rank Within Party', 'Party Code', 'Date of Appointment',\n",
      "       'Date of Termination', 'Senior Party Member', 'Committee Seniority',\n",
      "       'Committee Period of Service',\n",
      "       'Committee status at end of this Congress',\n",
      "       'Committee continuity of assignment in next Congress',\n",
      "       'Appointment Citation', 'Committee Name', 'State Code', 'District',\n",
      "       'State Name', 'Notes', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22',\n",
      "       'Unnamed: 23', 'Unnamed: 24'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554b6fb18d2d4567a6867048bdabbf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing subcommittees:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a101551b034a049be6ab9b0df2f65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing subcommittees:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported house_date_subcomm_mapper.pkl and senate_date_subcomm_mapper.pkl.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------- Helper Function --------\n",
    "def ensure_flat_list(x):\n",
    "    \"\"\"Ensure the input is returned as a flat list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        flat = []\n",
    "        for item in x:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        return flat\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "# -------- Membership Timeline by Subcommittee Function --------\n",
    "def create_date_subcomm_mapper(df, mapping_dict):\n",
    "    \"\"\"\n",
    "    Build a subcommittee membership timeline using the membership data in df.\n",
    "    \n",
    "    The DataFrame df must contain the columns:\n",
    "      - \"ID #\" (the congress internal member id)\n",
    "      - \"Date of Assignment\" (when the member joined the subcommittee)\n",
    "      - \"Date of Termination\" (when the member left; if missing, assumed active through 31 Dec 2021)\n",
    "      - \"Committee Name\" (the subcommittee)\n",
    "      \n",
    "    The mapping_dict is assumed to map a member’s \"ID #\" (as a string) \n",
    "    to a dictionary that has a key \"NODEID\" holding the Littlesis node id(s).\n",
    "    \n",
    "    For each subcommittee, the function creates a timeline dictionary where keys are event dates and values are \n",
    "    sorted lists of active Littlesis node IDs as of that date. Membership events are generated using:\n",
    "      - A \"join\" event on the assignment date.\n",
    "      - A \"leave\" event on the day after the termination date (or after Dec 31, 2021 if missing).\n",
    "      \n",
    "    Returns a dictionary mapping each subcommittee to its timeline.\n",
    "    \"\"\"\n",
    "    # Convert date columns\n",
    "    df[\"Date of Assignment\"] = pd.to_datetime(df[\"Date of Assignment\"], errors=\"coerce\", dayfirst=True)\n",
    "    df[\"Date of Termination\"] = pd.to_datetime(df[\"Date of Termination\"], errors=\"coerce\", dayfirst=True)\n",
    "    \n",
    "    subcomm_mapper = {}\n",
    "    subcommittees = df[\"Committee Name\"].unique()\n",
    "    \n",
    "    for subcomm in tqdm(subcommittees, desc=\"Processing subcommittees\"):\n",
    "        df_sub = df[df[\"Committee Name\"] == subcomm].copy()\n",
    "        events = []  # Will store tuples of (event_date, node, event_type)\n",
    "        \n",
    "        for _, row in df_sub.iterrows():\n",
    "            member = str(row[\"ID #\"]).strip()\n",
    "            # Look up the corresponding Littlesis node ids from mapping_dict;\n",
    "            # if no mapping, skip this row.\n",
    "            node_ids = mapping_dict.get(member, {}).get(\"NODEID\")\n",
    "            if node_ids is None:\n",
    "                continue\n",
    "            node_ids = ensure_flat_list(node_ids)\n",
    "            \n",
    "            assign_date = row[\"Date of Assignment\"]\n",
    "            term_date = row[\"Date of Termination\"]\n",
    "            # If termination is missing, use December 31, 2021\n",
    "            effective_term = term_date if pd.notna(term_date) else pd.Timestamp(\"2021-12-31\")\n",
    "            \n",
    "            if pd.notna(assign_date):\n",
    "                for node in node_ids:\n",
    "                    events.append((assign_date, node, \"join\"))\n",
    "            # Create leave events one day after effective termination.\n",
    "            for node in node_ids:\n",
    "                events.append((effective_term + pd.Timedelta(days=1), node, \"leave\"))\n",
    "        \n",
    "        # Sort events by date\n",
    "        events.sort(key=lambda x: x[0])\n",
    "        timeline = {}\n",
    "        active_nodes = set()\n",
    "        for event_date, node, event_type in events:\n",
    "            # Cast node to string (if not already) so that it is hashable.\n",
    "            node = str(node)\n",
    "            if event_type == \"join\":\n",
    "                active_nodes.add(node)\n",
    "            elif event_type == \"leave\":\n",
    "                active_nodes.discard(node)\n",
    "            # Snapshot the active nodes (sorted for consistency)\n",
    "            timeline[event_date] = sorted(active_nodes)\n",
    "        subcomm_mapper[subcomm] = timeline\n",
    "    return subcomm_mapper\n",
    "\n",
    "# -------- Main Code --------\n",
    "# Assume df_house and df_senate have already been read from \"house.csv\" and \"senate.csv\" respectively.\n",
    "# Restrict the columns to those needed.\n",
    "df_senate = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/senate_ass.csv\")\n",
    "print(df_house.columns)\n",
    "print(df_senate.columns)\n",
    "\n",
    "df_senate[\"Date of Assignment\"] = df_senate['Date of Appointment']\n",
    "# Restrict columns (and create a copy) for each chamber.\n",
    "df_house = df_house[[\"ID #\", 'Date of Assignment', \"Date of Termination\", \"Committee Name\"]].copy()\n",
    "df_senate = df_senate[[\"ID #\", 'Date of Assignment', 'Date of Termination', 'Committee Name']].copy()\n",
    "\n",
    "\n",
    "# Create the subcommittee membership timelines using the mapping_dict,\n",
    "# which should map internal \"ID #\" values (as strings) to a dictionary with key \"NODEID\".\n",
    "house_date_subcomm_mapper = create_date_subcomm_mapper(df_house, mapping_dict)\n",
    "senate_date_subcomm_mapper = create_date_subcomm_mapper(df_senate, mapping_dict)\n",
    "\n",
    "# Export the dictionaries to pickle files.\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/house_date_subcomm_mapper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(house_date_subcomm_mapper, f)\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/senate_date_subcomm_mapper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(senate_date_subcomm_mapper, f)\n",
    "\n",
    "print(\"Exported house_date_subcomm_mapper.pkl and senate_date_subcomm_mapper.pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342852d-ad5e-44e1-82d6-b488129b921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID #', 'Date of Assignment', 'Date of Termination', 'Committee Name'], dtype='object')\n",
      "Index(['Congress', 'Committee Code', 'ID #', 'Name', 'Maj/Min',\n",
      "       'Rank Within Party', 'Party Code', 'Date of Appointment',\n",
      "       'Date of Termination', 'Senior Party Member', 'Committee Seniority',\n",
      "       'Committee Period of Service',\n",
      "       'Committee status at end of this Congress',\n",
      "       'Committee continuity of assignment in next Congress',\n",
      "       'Appointment Citation', 'Committee Name', 'State Code', 'District',\n",
      "       'State Name', 'Notes', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22',\n",
      "       'Unnamed: 23', 'Unnamed: 24'],\n",
      "      dtype='object')\n",
      "House membership timeline and Senate membership timeline have been built and exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# ----- Helper Function -----\n",
    "def ensure_flat_list(x):\n",
    "    \"\"\"\n",
    "    If x is a list-of-lists or a single value, return a flat list.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        flat = []\n",
    "        for item in x:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "        return flat\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "# ----- Function to Create Membership Timeline -----\n",
    "def create_membership_timeline(df, mapping_dict, output_pickle_filename):\n",
    "    \"\"\"\n",
    "    Given a membership DataFrame (df) with at least the columns:\n",
    "      - \"ID #\"\n",
    "      - \"Date of Assignment\" and \"Date of Termination\"\n",
    "    \n",
    "    This function uses mapping_dict (which maps each congress member's internal \n",
    "    \"ID #\" to a dictionary that has a \"NODEID\" field containing Littlesis node id(s))\n",
    "    to create a timeline dictionary where keys are event dates and values \n",
    "    are lists of active Littlesis node IDs as of that date.\n",
    "    \n",
    "    If \"Date of Termination\" is NaN, it is substituted with December 31, 2021.\n",
    "    Then, the leave event is set to the following day (i.e. January 1, 2022).\n",
    "    \n",
    "    The resulting timeline is exported to a pickle file.\n",
    "    \"\"\"\n",
    "    # Copy and ensure proper date conversion.\n",
    "    df = df.copy()\n",
    "    df[\"Date of Assignment\"] = pd.to_datetime(df[\"Date of Assignment\"], errors=\"coerce\", dayfirst=True)\n",
    "    df[\"Date of Termination\"] = pd.to_datetime(df[\"Date of Termination\"], errors=\"coerce\", dayfirst=True)\n",
    "    \n",
    "    events = []  # Will store tuples: (event_date, node, event_type)\n",
    "    \n",
    "    # Iterate over each row.\n",
    "    for _, row in df.iterrows():\n",
    "        # Get the congress member's internal ID as string.\n",
    "        member_id = str(row[\"ID #\"]).strip()\n",
    "        # Look up the Littlesis node id(s) from mapping_dict.\n",
    "        node_ids = mapping_dict.get(member_id, {}).get(\"NODEID\")\n",
    "        if node_ids is None:\n",
    "            continue  # Skip if no mapping found.\n",
    "        # Ensure we have a flat list of node ids.\n",
    "        node_ids = ensure_flat_list(node_ids)\n",
    "        # Get the join (assignment) date.\n",
    "        assign_date = row[\"Date of Assignment\"]\n",
    "        # Use termination date if present; otherwise substitute with Dec 31, 2021.\n",
    "        term_date = row[\"Date of Termination\"]\n",
    "        effective_term_date = term_date if pd.notna(term_date) else pd.Timestamp(\"2021-12-31\")\n",
    "        \n",
    "        # Create join events (if assignment date is valid).\n",
    "        if pd.notna(assign_date):\n",
    "            for node in node_ids:\n",
    "                events.append((assign_date, node, \"join\"))\n",
    "        \n",
    "        # Create leave events one day after the effective termination date.\n",
    "        for node in node_ids:\n",
    "            events.append((effective_term_date + pd.Timedelta(days=1), node, \"leave\"))\n",
    "    \n",
    "    # Sort the events chronologically.\n",
    "    events.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Sweep through events, accumulating the active node IDs.\n",
    "    timeline = {}\n",
    "    active_nodes = set()\n",
    "    for event_date, node, event_type in events:\n",
    "        # Since node IDs must be hashable (e.g. a string or number), if not already, cast node to string.\n",
    "        node = str(node)\n",
    "        if event_type == \"join\":\n",
    "            active_nodes.add(node)\n",
    "        elif event_type == \"leave\":\n",
    "            active_nodes.discard(node)\n",
    "        # Save a snapshot of the current active nodes at this event date.\n",
    "        timeline[event_date] = sorted(active_nodes)\n",
    "    \n",
    "    # Export the timeline dictionary to a pickle file.\n",
    "    with open(output_pickle_filename, \"wb\") as f:\n",
    "        pickle.dump(timeline, f)\n",
    "    \n",
    "    return timeline\n",
    "\n",
    "# ----- Main Code to Build Both House and Senate Membership Timelines -----\n",
    "\n",
    "# (Assume that df_house and df_senate have been read already from their respective CSV files.)\n",
    "# For example, you might have:\n",
    "# df_house = pd.read_csv(\"house.csv\")\n",
    "df_senate = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/senate_ass.csv\")\n",
    "print(df_house.columns)\n",
    "print(df_senate.columns)\n",
    "\n",
    "df_senate[\"Date of Assignment\"] = df_senate[\"Date of Appointment\"]\n",
    "# Restrict columns (and create a copy) for each chamber.\n",
    "df_house = df_house[[\"ID #\", 'Date of Assignment', \"Date of Termination\", \"Committee Name\"]].copy()\n",
    "df_senate = df_senate[[\"ID #\", 'Date of Assignment', 'Date of Termination', 'Committee Name']].copy()\n",
    "\n",
    "# Now create the membership timelines using the mapping_dict that maps internal IDs to Littlesis NODEIDs.\n",
    "house_membership_timeline = create_membership_timeline(df_house, mapping_dict, f\"{NETWORK_RAW_FOLDERS}/house_membership_by_date.pkl\")\n",
    "senate_membership_timeline = create_membership_timeline(df_senate, mapping_dict, f\"{NETWORK_RAW_FOLDERS}/senate_membership_by_date.pkl\")\n",
    "\n",
    "print(\"House membership timeline and Senate membership timeline have been built and exported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea590cf-b2eb-4de2-8d3d-f7fc1a96ab5f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Building the TIC-to‑Subcommittee Dictionary**\n",
    "\n",
    "We start by importing the TIC‑to‑SIC data (for example, from **`TIC to SIC.xlsx`**). This file contains mapping data with at least the following columns:\n",
    "\n",
    "- **TIC**: the company’s TIC.\n",
    "- **Committee 1**, **Committee 2**, **Committee 3**, **Committee 4**: the subcommittee names associated with the company.\n",
    "\n",
    "**Example Table:**\n",
    "\n",
    "| **TIC** | **Committee 1**    | **Committee 2**    | **Committee 3** | **Committee 4** |\n",
    "|---------|--------------------|--------------------|-----------------|-----------------|\n",
    "| XYZ     | Subcommittee A     | Subcommittee B     | NaN             | NaN             |\n",
    "| ABC     | Subcommittee A     | NaN                | NaN             | NaN             |\n",
    "\n",
    "After grouping by **TIC** and filtering out any missing values (i.e. NaNs), we obtain—for example:\n",
    "\n",
    "> **tic_to_subcomm_mapper[\"XYZ\"] = {\"Subcommittee A\", \"Subcommittee B\"}**\n",
    "\n",
    "Next, we add a predefined set of **universal committees** to every company. For example, if:\n",
    "\n",
    "> **UNIVERSAL_COMMITTEES = {'BUDGET','COMMERCE','ECONOMIC (JOINT)','Energy and Commerce','Small Business','TAXATION (JOINT)','Ways and Means'}**\n",
    "\n",
    "then the final mapping for \"XYZ\" might be:\n",
    "\n",
    "> **tic_to_subcomm_mapper[\"XYZ\"] = {\"Subcommittee A\", \"Subcommittee B\", \"BUDGET\", \"COMMERCE\", \"ECONOMIC (JOINT)\", \"Energy and Commerce\", \"Small Business\", \"TAXATION (JOINT)\", \"Ways and Means\"}**\n",
    "\n",
    "*<mark>Note:</mark>* We store the committees as a **set** for uniqueness and efficient membership updates. If ordering is required later, you can always convert the set to a sorted list.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Exporting the Dictionaries to Pickle Files**\n",
    "\n",
    "Once you have built the mapping dictionaries (including **`tic_to_subcomm_mapper`** and the others you create later), you can export them to pickle files for later use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd23e19-2945-4e33-8a70-6b4f4d6a4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['Unnamed: 0', 'tic', 'gvkey', 'conm', 'cusip', 'cik', 'sic', 'naics', 'IDBFLAG', 'INCORP', 'GSECTOR', 'GGROUP', 'GIND', 'GSUBIND', 'Unnamed: 14', 'Committee 1', 'Committee 2', 'Committee 3', 'Committee 4', 'Unnamed: 19', 'Unnamed: 20', 'https://infotrie.com/docs/static-data/classifications/sd-1-2-gics-classifications/']\n",
      "\n",
      "Example mapping (first 3 entries):\n",
      "**AIR** -> {'TAXATION (JOINT)', 'Energy and Commerce', 'Ways and Means', 'ECONOMIC (JOINT)', 'Small Business', 'BUDGET', 'ARMED SERVICES', 'COMMERCE'}\n",
      "**AAL** -> {'Small Business', 'Energy and Commerce', 'Ways and Means', 'BUDGET', 'TAXATION (JOINT)', 'COMMERCE', 'ECONOMIC (JOINT)'}\n",
      "**CECO** -> {'Small Business', 'Energy and Commerce', 'Ways and Means', 'BUDGET', 'TAXATION (JOINT)', 'COMMERCE', 'ECONOMIC (JOINT)'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TIC-to-SIC file (Excel version)\n",
    "df_ticsic = pd.read_excel(f\"{NETWORK_RAW_FOLDERS}/TIC to SIC.xlsx\")\n",
    "print(\"Original columns:\", df_ticsic.columns.tolist())\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df_ticsic = df_ticsic[['tic', 'Committee 1', 'Committee 2', 'Committee 3', 'Committee 4']]\n",
    "\n",
    "# --- Step 3: Create the Mapping Dictionary ---\n",
    "# Convert each row’s committee values to strings and filter out NaN.\n",
    "tic_to_subcomm_mapper = (\n",
    "    df_ticsic\n",
    "    .set_index('tic')\n",
    "    .apply(lambda row: set(filter(pd.notna, row)), axis=1)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "UNIVERSAL_COMMITTEES = {'BUDGET','COMMERCE','ECONOMIC (JOINT)','Energy and Commerce','Small Business','TAXATION (JOINT)','Ways and Means'}\n",
    "\n",
    "for tic, comm_set in tic_to_subcomm_mapper.items():\n",
    "    # remove 0 or '0'\n",
    "    comm_set.discard(0)\n",
    "    comm_set.discard('0')\n",
    "    # add universal\n",
    "    comm_set.update(UNIVERSAL_COMMITTEES)\n",
    "\n",
    "\n",
    "# --- Optional: Display some examples ---\n",
    "print(\"\\nExample mapping (first 3 entries):\")\n",
    "for tic, committees in list(tic_to_subcomm_mapper.items())[:3]:\n",
    "    print(f\"**{tic}** -> {committees}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eca824-d64b-4157-95f6-e07b4ba6d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved tic_to_subcomm_mapper to 'tic_to_subcomm_mapper.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 4: Export the Dictionary to a Pickle File ---\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/tic_to_subcomm_mapper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tic_to_subcomm_mapper, f)\n",
    "print(\"\\nSaved tic_to_subcomm_mapper to 'tic_to_subcomm_mapper.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdfa2ac-a861-438a-9cfd-6e164fb533b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example mapping (first 3 entries):\n",
      "**AIR** -> {'Commerce, Science, and Transportation', 'Taxation (Joint)', 'Finance', 'Small Business and Entrepreneurship', 'Appropriations', 'Small Business', 'Banking, Housing, and Urban Affairs', 'Economic (Joint Committee)', 'Budget', 'ARMED SERVICES'}\n",
      "**AAL** -> {'Commerce, Science, and Transportation', 'Taxation (Joint)', 'Finance', 'Small Business and Entrepreneurship', 'Appropriations', 'Small Business', 'Banking, Housing, and Urban Affairs', 'Economic (Joint Committee)', 'Budget'}\n",
      "**CECO** -> {'Commerce, Science, and Transportation', 'Taxation (Joint)', 'Finance', 'Small Business and Entrepreneurship', 'Appropriations', 'Small Business', 'Banking, Housing, and Urban Affairs', 'Economic (Joint Committee)', 'Budget'}\n"
     ]
    }
   ],
   "source": [
    "## building House_membership_by_date\n",
    "\n",
    "df_sen_ticsic = pd.read_csv(f\"{NETWORK_RAW_FOLDERS}/Senate TIC to SIC.csv\")\n",
    "df_sen_ticsic = df_ticsic[['tic', 'Committee 1', 'Committee 2', 'Committee 3', 'Committee 4']]\n",
    "\n",
    "sen_tic_to_subcomm_mapper = (\n",
    "    df_sen_ticsic\n",
    "    .set_index('tic')\n",
    "    .apply(lambda row: set(filter(pd.notna, row)), axis=1)\n",
    "    .to_dict()\n",
    ")\n",
    "UNIVERSAL_COMMITTEES = {\n",
    "    \"Appropriations\",\n",
    "    \"Banking, Housing, and Urban Affairs\",\n",
    "    \"Budget\",\n",
    "    \"Commerce, Science, and Transportation\",\n",
    "    \"Economic (Joint Committee)\",\n",
    "    \"Finance\",\n",
    "    \"Small Business\",\n",
    "    \"Small Business and Entrepreneurship\",\n",
    "    \"Taxation (Joint)\"\n",
    "}\n",
    "\n",
    "for tic, comm_set in sen_tic_to_subcomm_mapper.items():\n",
    "    # remove 0 or '0'\n",
    "    comm_set.discard(0)\n",
    "    comm_set.discard('0')\n",
    "    # add universal\n",
    "    comm_set.update(UNIVERSAL_COMMITTEES)\n",
    "\n",
    "\n",
    "# --- Optional: Display some examples ---\n",
    "print(\"\\nExample mapping (first 3 entries):\")\n",
    "for tic, committees in list(sen_tic_to_subcomm_mapper.items())[:3]:\n",
    "    print(f\"**{tic}** -> {committees}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f5a7a-ef05-41ed-841e-608ea4123db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved sen_tic_to_subcomm_mapper to 'sen_tic_to_subcomm_mapper.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Export the Dictionary to a Pickle File ---\n",
    "with open(f\"{NETWORK_RAW_FOLDERS}/sen_tic_to_subcomm_mapper.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sen_tic_to_subcomm_mapper, f)\n",
    "print(\"\\nSaved sen_tic_to_subcomm_mapper to 'sen_tic_to_subcomm_mapper.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
