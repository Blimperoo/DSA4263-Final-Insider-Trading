{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Data Scraping \n",
    "Data is scraped from https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets\n",
    "\n",
    "This Jupyter Notebook scrapes, downloads, extracts, processes, and consolidates SEC insider transactions data from the SEC website. The final dataset is stored as separate csv files as well as a ZIP file, found in `data_untracked/sec_submissions/compiled/`. \n",
    "\n",
    "To know how to run the code for data, please refer to `datasets/README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Download Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FOLDER = '../../data_untracked/raw/sec_submissions'\n",
    "YEARS_THRESHOLD = (2005, 2024) # to match little sis network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 zip file links\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2024q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2024q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2024q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2024q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2023q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2023q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2023q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2023q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2022q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2022q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2022q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2022q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2021q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2021q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2021q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2021q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2020q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2020q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2020q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2020q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2019q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2019q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2019q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2019q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2018q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2018q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2018q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2018q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2017q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2017q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2017q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2017q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2016q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2016q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2016q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2016q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2015q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2015q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2015q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2015q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2014q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2014q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2014q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2014q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2013q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2013q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2013q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2013q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2012q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2012q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2012q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2012q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2011q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2011q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2011q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2011q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2010q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2010q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2010q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2010q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2009q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2009q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2009q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2009q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2008q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2008q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2008q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2008q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2007q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2007q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2007q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2007q1_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2006q4_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2006q3_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2006q2_form345.zip\n",
      "Downloaded: ../../data_untracked/raw/sec_submissions/interim/2006q1_form345.zip\n"
     ]
    }
   ],
   "source": [
    "# URL and headers for SEC data\n",
    "url = \"https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets\"\n",
    "headers = {\"User-Agent\": \"DSA4263 (dsa4263@gmail.com)\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error fetching page: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "zip_links = []\n",
    "\n",
    "# Look for all links that end with '.zip'\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if href.lower().endswith(\".zip\"):\n",
    "        # Normalize relative URLs if needed\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://www.sec.gov\" + href\n",
    "        # Extract a 4-digit year from the URL and filter (e.g., after 2010)\n",
    "        year_match = re.search(r\"(\\d{4})\", href)\n",
    "        if year_match:\n",
    "            try:\n",
    "                year = int(year_match.group(1))\n",
    "                if YEARS_THRESHOLD[0] <= year <= YEARS_THRESHOLD[1]:   \n",
    "                    zip_links.append(href)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"No year found in URL: {href}\")\n",
    "print(f\"Found {len(zip_links)} zip file links\")\n",
    "\n",
    "# Download each zip file\n",
    "for link in zip_links:\n",
    "    try:\n",
    "        r = requests.get(link, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {link}: {e}\")\n",
    "        continue\n",
    "\n",
    "    zip_filename = os.path.join(RAW_DATA_FOLDER, 'interim', link.split(\"/\")[-1])\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Downloaded: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Merge TSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied metadata/readme: insider_transactions_metadata.json\n",
      "Copied metadata/readme: insider_transactions_readme.htm\n"
     ]
    }
   ],
   "source": [
    "#temp folder \n",
    "TEMP_EXTRACTED = RAW_DATA_FOLDER+\"/temp_extracted\"\n",
    "# Final folder for merged output\n",
    "FINAL_FOLDER = RAW_DATA_FOLDER+\"/compiled\"\n",
    "os.makedirs(FINAL_FOLDER, exist_ok=True)\n",
    "\n",
    "# Dictionary to store DataFrames keyed by the TSV filename (e.g., \"DERIV_HOLDING.tsv\")\n",
    "merged_data = {}\n",
    "# Keep track of whether we've copied the metadata and readme files yet\n",
    "metadata_copied = 0\n",
    "#large files to exclude\n",
    "excluded_files = ['owner_signature.tsv', 'footnotes.tsv']\n",
    "\n",
    "# Process each downloaded zip file from the RAW_DATA folder\n",
    "zip_files = glob.glob(os.path.join(RAW_DATA_FOLDER+\"/interim\", \"*.zip\"))\n",
    "\n",
    "for zip_path in zip_files:\n",
    "    \n",
    "    #temp file\n",
    "    if os.path.exists(TEMP_EXTRACTED):\n",
    "        shutil.rmtree(TEMP_EXTRACTED)\n",
    "    os.makedirs(TEMP_EXTRACTED, exist_ok=True)\n",
    "\n",
    "    # Extract the zip contents to temp\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(TEMP_EXTRACTED)\n",
    "\n",
    "    # Process extracted files: merge TSVs and copy metadata/readme\n",
    "    for root, dirs, files in os.walk(TEMP_EXTRACTED):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            # If it's a TSV, load and merge it\n",
    "            if filename.lower().endswith(\".tsv\"):\n",
    "                if filename.lower() not in excluded_files: \n",
    "                    try:\n",
    "                        df = pd.read_csv(filepath, sep=\"\\t\", low_memory=False)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filepath}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if filename not in merged_data:\n",
    "                        merged_data[filename] = df\n",
    "                    else:\n",
    "                        merged_data[filename] = pd.concat([merged_data[filename], df], ignore_index=True)\n",
    "\n",
    "            # If it's metadata or readme, copy only once\n",
    "            elif filename in [\"insider_transactions_metadata.json\", \"insider_transactions_readme.htm\"]:\n",
    "                if metadata_copied < 2:\n",
    "                    dest_path = os.path.join(FINAL_FOLDER, filename)\n",
    "                    shutil.copy2(filepath, dest_path)\n",
    "                    print(f\"Copied metadata/readme: {filename}\")\n",
    "                    metadata_copied += 1\n",
    "\n",
    "    # Remove the temporary extraction folder\n",
    "    shutil.rmtree(TEMP_EXTRACTED, ignore_errors=True)\n",
    "\n",
    "# Write out the merged TSV files into the final folder\n",
    "for tsv_name, df in merged_data.items():\n",
    "    output_path = os.path.join(FINAL_FOLDER, tsv_name)\n",
    "    df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Merged TSV saved: {output_path}\")\n",
    "\n",
    "print(\"All TSV files have been merged.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load metadata and build a mapping from TSV filename `{col_name -> datatype}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'excluded_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtables\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     tsv_filename \u001b[38;5;241m=\u001b[39m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# e.g. \"OWNER_SIGNATURE.tsv\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tsv_filename\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexcluded_files\u001b[49m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     col_mappings \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'excluded_files' is not defined"
     ]
    }
   ],
   "source": [
    "with open(FINAL_FOLDER+\"/insider_transactions_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "conversion_mapping = {}\n",
    "for table in metadata[\"tables\"]:\n",
    "    tsv_filename = table[\"url\"]  # e.g. \"OWNER_SIGNATURE.tsv\"\n",
    "    if tsv_filename.lower() in excluded_files:\n",
    "        continue\n",
    "    col_mappings = {}\n",
    "    for col in table[\"tableSchema\"][\"columns\"]:\n",
    "        # e.g. col[\"name\"] might be \"ACCESSION_NUMBER\"\n",
    "        col_mappings[col[\"name\"]] = col[\"datatype\"]\n",
    "    conversion_mapping[tsv_filename] = col_mappings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper function for converting datatypes \n",
    "# -----------------------------------------------------------------------------\n",
    "def convert_value(series, datatype):\n",
    "    base = datatype[\"base\"].lower()\n",
    "    if \"number\" in base:\n",
    "        # Convert to numeric; non-convertible values become NaN\n",
    "        return pd.to_numeric(series, errors=\"coerce\")\n",
    "    elif \"date (dd-mon-yyyy)\" in base:\n",
    "        # Convert to datetime using format \"DD-MON-YYYY\" (e.g. \"01-JAN-2020\")\n",
    "        return pd.to_datetime(series, format=\"%d-%b-%Y\", errors=\"coerce\")\n",
    "    else:\n",
    "        #Ignore for rest\n",
    "        return series.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each TSV file: read, convert, then save as CSV & delete TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NONDERIV_TRANS.tsv...\n",
      "  Saved converted data to data/interim/NONDERIV_TRANS.csv\n",
      "  Deleted original file: data/interim/NONDERIV_TRANS.tsv\n",
      "\n",
      "Processing NONDERIV_HOLDING.tsv...\n",
      "  Saved converted data to data/interim/NONDERIV_HOLDING.csv\n",
      "  Deleted original file: data/interim/NONDERIV_HOLDING.tsv\n",
      "\n",
      "Processing REPORTINGOWNER.tsv...\n",
      "  Saved converted data to data/interim/REPORTINGOWNER.csv\n",
      "  Deleted original file: data/interim/REPORTINGOWNER.tsv\n",
      "\n",
      "Processing DERIV_HOLDING.tsv...\n",
      "  Saved converted data to data/interim/DERIV_HOLDING.csv\n",
      "  Deleted original file: data/interim/DERIV_HOLDING.tsv\n",
      "\n",
      "Processing DERIV_TRANS.tsv...\n",
      "  Saved converted data to data/interim/DERIV_TRANS.csv\n",
      "  Deleted original file: data/interim/DERIV_TRANS.tsv\n",
      "\n",
      "Processing SUBMISSION.tsv...\n",
      "  Saved converted data to data/interim/SUBMISSION.csv\n",
      "  Deleted original file: data/interim/SUBMISSION.tsv\n"
     ]
    }
   ],
   "source": [
    "tsv_files = glob.glob(os.path.join(FINAL_FOLDER, \"*.tsv\"))\n",
    "problems = []\n",
    "\n",
    "for tsv_file in tsv_files:\n",
    "    df = pd.read_csv(tsv_file, sep=\"\\t\", dtype=str,low_memory=False) #low_memory params used here for accuracy \n",
    "    filename = os.path.basename(tsv_file)\n",
    "    if filename.lower() in excluded_files: \n",
    "        continue\n",
    "    print(f\"\\nProcessing {filename}...\")\n",
    "\n",
    "    #Debugging\n",
    "    #print(\"  Columns found in TSV:\", df.columns.tolist())\n",
    "\n",
    "    # Conversion of datatypes \n",
    "    if filename in conversion_mapping:\n",
    "        meta_for_file = conversion_mapping[filename]\n",
    "        for col_name, datatype_info in meta_for_file.items():\n",
    "            if col_name in df.columns:\n",
    "                df[col_name] = convert_value(df[col_name], datatype_info)\n",
    "            else:\n",
    "                problems.append(f\"Column '{col_name}' not found in {filename}.\")\n",
    "    else: \n",
    "        print(f\"  No metadata mapping found for {filename}.\")\n",
    "\n",
    "    # Debugging\n",
    "    # print(\"  Data types after conversion:\")\n",
    "    # print(df.dtypes)\n",
    "\n",
    "    # Store as csv files\n",
    "    csv_filename = os.path.splitext(tsv_file)[0] + \".csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"  Saved converted data to {csv_filename}\")\n",
    "\n",
    "    # Delete original tsv\n",
    "    os.remove(tsv_file)\n",
    "    print(f\"  Deleted original file: {tsv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip Folder, and remove the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip file: ../../data_untracked/raw/sec_submissions/compiled/sec_submissions_raw.zip\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Zip the Final Data Folder\n",
    "# -------------------------------\n",
    "ZIP_FILENAME = FINAL_FOLDER+\"/sec_submissions_raw.zip\"\n",
    "with zipfile.ZipFile(ZIP_FILENAME, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(FINAL_FOLDER):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, FINAL_FOLDER)\n",
    "            zipf.write(filepath, arcname)\n",
    "print(f\"Created zip file: {ZIP_FILENAME}\")\n",
    "\n",
    "## Keep the csv files for sec_data_merging.ipynb \n",
    "# -------------------------------\n",
    "# Clean Up: Remove all files from ../../data/interim \n",
    "# -------------------------------\n",
    "#for root, dirs, files in os.walk(final_folder):\n",
    "#    for file in files:\n",
    "#        if file not in [\".gitkeep\", \"FINAL_RAW_DATA.zip\"]:\n",
    "#            file_path = os.path.join(root, file)\n",
    "#            os.remove(file_path)\n",
    "#            print(f\"Removed file: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
