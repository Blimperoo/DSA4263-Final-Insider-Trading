{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Data Scraping \n",
    "Data is scraped from https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets\n",
    "\n",
    "This Jupyter Notebook scrapes, downloads, extracts, processes, and consolidates SEC insider transactions data from the SEC website. The final dataset is stored as a ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Download Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'data/raw' # store raw zip files\n",
    "YEARS_THRESHOLD = (2005, 2021) # to match little sis network data\n",
    "\n",
    "# URL and headers for SEC data\n",
    "url = \"https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets\"\n",
    "headers = {\"User-Agent\": \"DSA4263 (dsa4263@gmail.com)\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error fetching page: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "zip_links = []\n",
    "\n",
    "# Look for all links that end with '.zip'\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if href.lower().endswith(\".zip\"):\n",
    "        # Normalize relative URLs if needed\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://www.sec.gov\" + href\n",
    "        # Extract a 4-digit year from the URL and filter (e.g., after 2010)\n",
    "        year_match = re.search(r\"(\\d{4})\", href)\n",
    "        if year_match:\n",
    "            try:\n",
    "                year = int(year_match.group(1))\n",
    "                if YEARS_THRESHOLD[0] <= year <= YEARS_THRESHOLD[1]:   \n",
    "                    zip_links.append(href)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"No year found in URL: {href}\")\n",
    "print(f\"Found {len(zip_links)} zip file links\")\n",
    "\n",
    "# Download each zip file\n",
    "for link in zip_links:\n",
    "    try:\n",
    "        r = requests.get(link, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {link}: {e}\")\n",
    "        continue\n",
    "\n",
    "    zip_filename = os.path.join(raw_data_folder, link.split(\"/\")[-1])\n",
    "    with open(zip_filename, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Downloaded: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Merge TSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied metadata/readme: insider_transactions_metadata.json\n",
      "Copied metadata/readme: insider_transactions_readme.htm\n",
      "Merged TSV saved: data/interim/NONDERIV_TRANS.tsv\n",
      "Merged TSV saved: data/interim/NONDERIV_HOLDING.tsv\n",
      "Merged TSV saved: data/interim/REPORTINGOWNER.tsv\n",
      "Merged TSV saved: data/interim/DERIV_HOLDING.tsv\n",
      "Merged TSV saved: data/interim/DERIV_TRANS.tsv\n",
      "Merged TSV saved: data/interim/SUBMISSION.tsv\n",
      "All TSV files have been merged.\n"
     ]
    }
   ],
   "source": [
    "#temp folder \n",
    "temp_extracted = \"temp_extracted\"\n",
    "# Final folder for merged output\n",
    "final_folder = \"data/interim\"\n",
    "os.makedirs(final_folder, exist_ok=True)\n",
    "\n",
    "# Dictionary to store DataFrames keyed by the TSV filename (e.g., \"DERIV_HOLDING.tsv\")\n",
    "merged_data = {}\n",
    "# Keep track of whether we've copied the metadata and readme files yet\n",
    "metadata_copied = 0\n",
    "#large files to exclude\n",
    "excluded_files = ['owner_signature.tsv', 'footnotes.tsv']\n",
    "\n",
    "# Process each downloaded zip file from the RAW_DATA folder\n",
    "zip_files = glob.glob(os.path.join(raw_data_folder, \"*.zip\"))\n",
    "\n",
    "for zip_path in zip_files:\n",
    "    \n",
    "    #temp file\n",
    "    if os.path.exists(temp_extracted):\n",
    "        shutil.rmtree(temp_extracted)\n",
    "    os.makedirs(temp_extracted, exist_ok=True)\n",
    "\n",
    "    # Extract the zip contents to temp\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(temp_extracted)\n",
    "\n",
    "    # Process extracted files: merge TSVs and copy metadata/readme\n",
    "    for root, dirs, files in os.walk(temp_extracted):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            # If it's a TSV, load and merge it\n",
    "            if filename.lower().endswith(\".tsv\"):\n",
    "                if filename.lower() not in excluded_files: \n",
    "                    try:\n",
    "                        df = pd.read_csv(filepath, sep=\"\\t\", low_memory=False)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filepath}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if filename not in merged_data:\n",
    "                        merged_data[filename] = df\n",
    "                    else:\n",
    "                        merged_data[filename] = pd.concat([merged_data[filename], df], ignore_index=True)\n",
    "\n",
    "            # If it's metadata or readme, copy only once\n",
    "            elif filename in [\"insider_transactions_metadata.json\", \"insider_transactions_readme.htm\"]:\n",
    "                if metadata_copied < 2:\n",
    "                    dest_path = os.path.join(final_folder, filename)\n",
    "                    shutil.copy2(filepath, dest_path)\n",
    "                    print(f\"Copied metadata/readme: {filename}\")\n",
    "                    metadata_copied += 1\n",
    "\n",
    "    # Remove the temporary extraction folder\n",
    "    shutil.rmtree(temp_extracted, ignore_errors=True)\n",
    "\n",
    "# Write out the merged TSV files into the final folder\n",
    "for tsv_name, df in merged_data.items():\n",
    "    output_path = os.path.join(final_folder, tsv_name)\n",
    "    df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Merged TSV saved: {output_path}\")\n",
    "\n",
    "print(\"All TSV files have been merged.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load metadata and build a mapping from TSV filename `{col_name -> datatype}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/interim/insider_transactions_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "conversion_mapping = {}\n",
    "for table in metadata[\"tables\"]:\n",
    "    tsv_filename = table[\"url\"]  # e.g. \"OWNER_SIGNATURE.tsv\"\n",
    "    if tsv_filename.lower() in excluded_files:\n",
    "        continue\n",
    "    col_mappings = {}\n",
    "    for col in table[\"tableSchema\"][\"columns\"]:\n",
    "        # e.g. col[\"name\"] might be \"ACCESSION_NUMBER\"\n",
    "        col_mappings[col[\"name\"]] = col[\"datatype\"]\n",
    "    conversion_mapping[tsv_filename] = col_mappings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper function for converting datatypes \n",
    "# -----------------------------------------------------------------------------\n",
    "def convert_value(series, datatype):\n",
    "    base = datatype[\"base\"].lower()\n",
    "    if \"number\" in base:\n",
    "        # Convert to numeric; non-convertible values become NaN\n",
    "        return pd.to_numeric(series, errors=\"coerce\")\n",
    "    elif \"date (dd-mon-yyyy)\" in base:\n",
    "        # Convert to datetime using format \"DD-MON-YYYY\" (e.g. \"01-JAN-2020\")\n",
    "        return pd.to_datetime(series, format=\"%d-%b-%Y\", errors=\"coerce\")\n",
    "    else:\n",
    "        #Ignore for rest\n",
    "        return series.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each TSV file: read, convert, then save as CSV & delete TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NONDERIV_TRANS.tsv...\n",
      "  Saved converted data to data/interim/NONDERIV_TRANS.csv\n",
      "  Deleted original file: data/interim/NONDERIV_TRANS.tsv\n",
      "\n",
      "Processing NONDERIV_HOLDING.tsv...\n",
      "  Saved converted data to data/interim/NONDERIV_HOLDING.csv\n",
      "  Deleted original file: data/interim/NONDERIV_HOLDING.tsv\n",
      "\n",
      "Processing REPORTINGOWNER.tsv...\n",
      "  Saved converted data to data/interim/REPORTINGOWNER.csv\n",
      "  Deleted original file: data/interim/REPORTINGOWNER.tsv\n",
      "\n",
      "Processing DERIV_HOLDING.tsv...\n",
      "  Saved converted data to data/interim/DERIV_HOLDING.csv\n",
      "  Deleted original file: data/interim/DERIV_HOLDING.tsv\n",
      "\n",
      "Processing DERIV_TRANS.tsv...\n",
      "  Saved converted data to data/interim/DERIV_TRANS.csv\n",
      "  Deleted original file: data/interim/DERIV_TRANS.tsv\n",
      "\n",
      "Processing SUBMISSION.tsv...\n",
      "  Saved converted data to data/interim/SUBMISSION.csv\n",
      "  Deleted original file: data/interim/SUBMISSION.tsv\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"data/interim\"\n",
    "tsv_files = glob.glob(os.path.join(data_folder, \"*.tsv\"))\n",
    "problems = []\n",
    "\n",
    "for tsv_file in tsv_files:\n",
    "    df = pd.read_csv(tsv_file, sep=\"\\t\", dtype=str,low_memory=False) #low_memory params used here for accuracy \n",
    "    filename = os.path.basename(tsv_file)\n",
    "    if filename.lower() in excluded_files: \n",
    "        continue\n",
    "    print(f\"\\nProcessing {filename}...\")\n",
    "\n",
    "    #Debugging\n",
    "    #print(\"  Columns found in TSV:\", df.columns.tolist())\n",
    "\n",
    "    # Conversion of datatypes \n",
    "    if filename in conversion_mapping:\n",
    "        meta_for_file = conversion_mapping[filename]\n",
    "        for col_name, datatype_info in meta_for_file.items():\n",
    "            if col_name in df.columns:\n",
    "                df[col_name] = convert_value(df[col_name], datatype_info)\n",
    "            else:\n",
    "                problems.append(f\"Column '{col_name}' not found in {filename}.\")\n",
    "    else: \n",
    "        print(f\"  No metadata mapping found for {filename}.\")\n",
    "\n",
    "    # Debugging\n",
    "    # print(\"  Data types after conversion:\")\n",
    "    # print(df.dtypes)\n",
    "\n",
    "    #Store as csv files\n",
    "    csv_filename = os.path.splitext(tsv_file)[0] + \".csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"  Saved converted data to {csv_filename}\")\n",
    "\n",
    "    #Delete original tsv\n",
    "    os.remove(tsv_file)\n",
    "    print(f\"  Deleted original file: {tsv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip Folder, and remove the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip file: data/interim/FINAL_RAW_DATA.zip\n",
      "Removed file: data/interim/SUBMISSION.csv\n",
      "Removed file: data/interim/DERIV_TRANS.csv\n",
      "Removed file: data/interim/insider_transactions_metadata.json\n",
      "Removed file: data/interim/insider_transactions_readme.htm\n",
      "Removed file: data/interim/DERIV_HOLDING.csv\n",
      "Removed file: data/interim/REPORTINGOWNER.csv\n",
      "Removed file: data/interim/NONDERIV_TRANS.csv\n",
      "Removed file: data/interim/NONDERIV_HOLDING.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Zip the Final Data Folder\n",
    "# -------------------------------\n",
    "zip_filename = \"data/interim/FINAL_RAW_DATA.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(final_folder):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, final_folder)\n",
    "            zipf.write(filepath, arcname)\n",
    "print(f\"Created zip file: {zip_filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Clean Up: Remove all files from data/interim \n",
    "# -------------------------------\n",
    "for root, dirs, files in os.walk(final_folder):\n",
    "    for file in files:\n",
    "        if file not in [\".gitkeep\", \"FINAL_RAW_DATA.zip\"]:\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed file: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
