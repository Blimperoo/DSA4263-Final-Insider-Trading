{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Creation - Abnormal Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RAW_DATA_FOLDER_AR = \"../../data_untracked/raw/abnormal_returns\" \n",
    "PROCESSED_DATA_FOLDER = \"../../data_untracked/processed\"\n",
    "\n",
    "# Change DEBUG to False to see less printouts\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    import pandas_market_calendars as mcal\n",
    "    nyse = mcal.get_calendar(\"NYSE\") # Get the NYSE trading calendar to calculate number of trading days to check validility of CRSP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required datasets\n",
    "1. `all_transactions_final`: output of `sec_data_merging.ipynb`\n",
    "2. `all_beta_daily`: beta and return values from crsp data for all tickers \n",
    "3. `risk_free_rate_daily`: risk free rate from 2005 to 2021, downloaded from https://fred.stlouisfed.org/series/DTB3\n",
    "    * This rate is the same for ALL tickers, joined on the transaction date\n",
    "4. `unique_ticker_trans_8above`: unique list of tickers with more than 8 transactions from Form 4 merged data\n",
    "    * This list is to create abnormal return calculations for each ticker, in order for computational efficiency\n",
    "5. **Note that** `stock_price`: is the daily stock price for each ticker, loaded for each ticker in abnormal return calculation because the data files are stored as '<ticker>.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254197, 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads the output of sec_data_merging.ipynb merging notebook\n",
    "all_transactions_final = pd.read_csv(f\"{PROCESSED_DATA_FOLDER}/all_transactions_merged.csv\")\n",
    "all_transactions_final['TRANS_DATE'] = pd.to_datetime(all_transactions_final['TRANS_DATE'], errors='coerce')\n",
    "all_transactions_final.dropna(subset=['TRANS_PRICEPERSHARE'], inplace=True)\n",
    "all_transactions_final.shape # (2546985, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22639404, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads the beta and return values from crsp data\n",
    "all_beta_daily = pd.read_csv(f\"{RAW_DATA_FOLDER_AR}/crsp/beta_daily.csv\")\n",
    "all_beta_daily['DATE'] = pd.to_datetime(all_beta_daily['DATE'], errors='coerce')\n",
    "all_beta_daily.rename(columns={'RET':'ret_beta'}, inplace=True) # rename RET from beta dataset to ret_beta to avoid confusion\n",
    "all_beta_daily.shape # (22639404, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6523, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loads the risk free rate from 2005 to 2021, downloaded from https://fred.stlouisfed.org/series/DTB3 \n",
    "risk_free_rate_daily = pd.read_csv(f\"{RAW_DATA_FOLDER_AR}/fred/risk_free_rate_daily.csv\")\n",
    "risk_free_rate_daily.rename(columns={'observation_date':'date', 'DTB3':'risk_free_rate'}, inplace=True)\n",
    "risk_free_rate_daily['date'] = pd.to_datetime(risk_free_rate_daily['date'], errors='coerce')\n",
    "risk_free_rate_daily.shape # (4435, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the unique list of tickers with more than 8 transactions from Form 4 merged data\n",
    "unique_ticker_trans_8above = pd.read_csv(f'{PROCESSED_DATA_FOLDER}/unique_names_trans_8above.csv')\n",
    "ticker_list = unique_ticker_trans_8above[['ISSUERTRADINGSYMBOL']].drop_duplicates().values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters for CAR\n",
    "- this can be modified based on literature reviews and our EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_BEFORE_TRANSACTION = 5\n",
    "DAYS_AFTER_TRANSACTION = 0\n",
    "MIN_AR_VALUES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute categorical numerical column where NA values are between the same values\n",
    "def categorical_impute(series):\n",
    "    \"\"\"\n",
    "    Imputes missing values by carrying forward the same value if the before and after values are identical.\n",
    "    Otherwise, leaves NaN.\n",
    "    \"\"\"\n",
    "    series = series.copy()  # Avoid modifying original data\n",
    "    for i in range(1, len(series) - 1):\n",
    "        if pd.isna(series[i]):  # If current value is NaN\n",
    "            prev_idx = i - 1\n",
    "            next_idx = i + 1\n",
    "\n",
    "            # Find the next non-NaN value\n",
    "            while next_idx < len(series) and pd.isna(series[next_idx]):\n",
    "                next_idx += 1\n",
    "\n",
    "            # If both a previous and next valid value exist and they are the same, impute\n",
    "            if prev_idx >= 0 and next_idx < len(series) and series[prev_idx] == series[next_idx]:\n",
    "                series[i] = series[prev_idx]\n",
    "\n",
    "    return series\n",
    "\n",
    "# Function to perform mean imputation for consecutive NaN values\n",
    "def mean_impute_between_values(series):\n",
    "    \"\"\"\n",
    "    Imputes missing values by taking the mean of the nearest non-NaN values before and after.\n",
    "    \"\"\"\n",
    "    series = series.copy()  # Avoid modifying original data\n",
    "    for i in range(1, len(series) - 1):\n",
    "        if pd.isna(series[i]):  # If current value is NaN\n",
    "            prev_idx = i - 1\n",
    "            next_idx = i + 1\n",
    "\n",
    "            # Find the next non-NaN value\n",
    "            while next_idx < len(series) and pd.isna(series[next_idx]):\n",
    "                next_idx += 1\n",
    "\n",
    "            # If both a previous and next valid value exist, take their mean\n",
    "            if prev_idx >= 0 and next_idx < len(series) and not pd.isna(series[prev_idx]) and not pd.isna(series[next_idx]):\n",
    "                series[i] = (series[prev_idx] + series[next_idx]) / 2\n",
    "\n",
    "    return series\n",
    "\n",
    "## Compute cumulative abnormal return (CAR) within the given window\n",
    "def calculate_cumulative_abnormal_return(trans_date, permno, abnormal_ret_data, \n",
    "                                         days_before = DAYS_BEFORE_TRANSACTION, \n",
    "                                         days_after = DAYS_AFTER_TRANSACTION, \n",
    "                                         min_ar_values = MIN_AR_VALUES):\n",
    "    '''\n",
    "    This is meant to be used in a .apply() function, for each row of transaction data. \n",
    "    For each transaction date, retrieve the PERMNO and find a subset of abnormal_returns during the event window, and sum abnormal_ret to get CAR\n",
    "    '''\n",
    "    # Define the event window (-6 to +2 days)\n",
    "    start_date = trans_date - pd.Timedelta(days=days_before)\n",
    "    end_date = trans_date + pd.Timedelta(days=days_after)\n",
    "\n",
    "    # Filter abnormal return data based on date range and matching PERMNO\n",
    "    subset = abnormal_ret_data[(abnormal_ret_data['date'] >= start_date) & \n",
    "                            (abnormal_ret_data['date'] <= end_date) & \n",
    "                            (abnormal_ret_data['PERMNO'] == permno)]\n",
    "\n",
    "    # Ensure at least X non-NA values before summing\n",
    "    count = subset['abnormal_ret'].count()\n",
    "    if count >= min_ar_values:\n",
    "        ### Initially wanted to also return standard deviation / Z score\n",
    "        #std = np.std(subset['abnormal_ret'])\n",
    "        #if std == 0: # unlikely, but it may be possible that there is no change in abnormal_return over the time period\n",
    "        #    t_test = None\n",
    "        #else: \n",
    "        #    t_test = (subset['abnormal_ret'].sum()/count) / (np.std(subset['abnormal_ret']) / np.sqrt(count))\n",
    "\n",
    "        return subset['abnormal_ret'].sum()\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return None # Return None if there aren't at least X valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to calculate abnormal returns\n",
    "def update_trans_ticker_and_create_ar_compare(ticker:str, permno:float,trans_df_permno, stock_price_df, beta_ticker_df, \n",
    "                                              risk_free_rate_df=risk_free_rate_daily, days_before=DAYS_BEFORE_TRANSACTION, days_after=DAYS_AFTER_TRANSACTION):\n",
    "    '''\n",
    "    This is meant to be run for each (TICKER, PERMNO) pair\n",
    "\n",
    "    Parameters:\n",
    "    ticker: TICKER of the stock in string\n",
    "    permno: PERMNO of the ticker in string\n",
    "    trans_df_permno: DataFrame of transactions for the specific (ticker and) PERMNO. All permnos are the same in this dataframe\n",
    "    stock_price_df: DataFrame of stock prices for (TICKER, PERMNO) pair\n",
    "    beta_ticker_df: DataFrame of beta values for (TICKER, PERMNO) pair\n",
    "    risk_free_rate_df: DataFrame of risk free rate values\n",
    "\n",
    "    Returns:\n",
    "    ticker_ar: DataFrame with date -6 and +2 of the permno's date range, with daily stock price, beta values, risk free rate and abnormal returns \n",
    "    trans_ticker_price: DataFrame of transactions with abnormal returns and cumulative abnormal returns based on hyperparameters (currently -6, +2)\n",
    "    '''\n",
    "\n",
    "    # Get start and end of the transaction date, to then create a range of abnormal returns\n",
    "    start, end = str(min(trans_df_permno['TRANS_DATE']))[:10], str(max(trans_df_permno['TRANS_DATE']))[:10]\n",
    "\n",
    "    ## The date range here can be modified!!!\n",
    "    start_new, end_new = pd.to_datetime(start) - pd.to_timedelta(days_before, unit='d'), pd.to_datetime(end) + pd.to_timedelta(days_after, unit='d')\n",
    "\n",
    "    # Create a new dataframe to store values associated with daily transactions and abnormal returns\n",
    "    ticker_ar = pd.DataFrame({'date': pd.date_range(start=start_new, end=end_new)})\n",
    "    ticker_ar['date'] = pd.to_datetime(ticker_ar['date'], errors='coerce')\n",
    "\n",
    "    if DEBUG: ticker_ar_len = ticker_ar.shape[0]\n",
    "\n",
    "    # Filter stock price data and beta data to match PERMNO\n",
    "    stock_price_filtered = stock_price_df[(stock_price_df['PERMNO'] == permno)]\n",
    "    beta_ticker_filtered = beta_ticker_df[(beta_ticker_df['PERMNO'] == permno)]\n",
    "    \n",
    "    # Merge date range with stock price, beta values and risk free rate\n",
    "    ticker_ar = ticker_ar.merge(stock_price_filtered[['date', 'TICKER', 'RET', 'RETX', 'VOL']].drop_duplicates(), on='date', how='left')\n",
    "    #if DEBUG and ticker_ar_len != ticker_ar.shape[0]: print(f\"{ticker} Missing rows after merge with stock price: before {ticker_ar_len}, after {ticker_ar.shape[0]}\")\n",
    "    ticker_ar = ticker_ar.merge(beta_ticker_filtered[['DATE', 'PERMNO', 'ret_beta','alpha', 'b_mkt']], left_on='date', right_on='DATE', how='left')\n",
    "    if DEBUG and abs(ticker_ar_len - ticker_ar.shape[0]) > 10: print(f\"{permno} Missing rows after merge with beta: before {ticker_ar_len}, after {ticker_ar.shape[0]}\")\n",
    "    ticker_ar = ticker_ar.merge(risk_free_rate_df, on='date', how='left')\n",
    "\n",
    "    # Add relevant columns for identification when merging to overall dataset\n",
    "    ticker_ar['TICKER'] = ticker\n",
    "    ticker_ar['PERMNO'] = permno\n",
    "\n",
    "    # Transform actual_returns from beta data (object type) to float type\n",
    "    ticker_ar['actual_ret'] = ticker_ar['ret_beta'].str.replace('%', '') #.astype(float, errors='coerce') / 100\n",
    "    ticker_ar['actual_ret'] = pd.to_numeric(ticker_ar['actual_ret'], errors='coerce') / 100\n",
    "\n",
    "    # Impute: if theres values IN BETWEEN non-NA values, then impute mean. Start and End NA values ignore (the most likely case is because there was no stock data yet)\n",
    "    ticker_ar['actual_ret'] = mean_impute_between_values(ticker_ar['actual_ret'].copy())\n",
    "    ticker_ar['b_mkt'] = mean_impute_between_values(ticker_ar['b_mkt'].copy())\n",
    "    ticker_ar['risk_free_rate'] = mean_impute_between_values(ticker_ar['risk_free_rate'].copy())\n",
    "\n",
    "    # Calculate expected return and abnormal return\n",
    "    ticker_ar['expected_ret'] = ticker_ar['risk_free_rate'] + ticker_ar['b_mkt'] * (ticker_ar['actual_ret'] - ticker_ar['risk_free_rate'])\n",
    "    ticker_ar['abnormal_ret'] = ticker_ar['actual_ret'] - ticker_ar['expected_ret']\n",
    "\n",
    "    trans_ticker_price = trans_df_permno.merge(ticker_ar[['date','actual_ret','b_mkt','risk_free_rate','expected_ret','abnormal_ret']], left_on='TRANS_DATE', right_on='date', how='left')\n",
    "\n",
    "    # Apply function to calculate CAR for [-5, 0] days from each transaction date (several days before)\n",
    "    trans_ticker_price['CAR_5_before'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         5, 0, 4),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [0, 5] days from each transaction date (several days after)\n",
    "    trans_ticker_price['CAR_5_after'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         0, 5, 4),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [-30, 0] days from each transaction date (a month before)\n",
    "    trans_ticker_price['CAR_30_before'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         30, 0, 25),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [0, 30] days from each transaction date (a month after)\n",
    "    trans_ticker_price['CAR_30_after'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         0, 30, 25),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [-60, 0] days from each transaction date (a month before)\n",
    "    trans_ticker_price['CAR_60_before'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         60, 0, 45),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [0, 60] days from each transaction date (a month after)\n",
    "    trans_ticker_price['CAR_60_after'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         0, 60, 45),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [120, 0] days from each transaction date (a month before)\n",
    "    trans_ticker_price['CAR_120_before'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         120, 0, 100),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    # Apply function to calculate CAR for [0, 120] days from each transaction date (a month after)\n",
    "    trans_ticker_price['CAR_120_after'] = trans_ticker_price.apply(\n",
    "        lambda row: calculate_cumulative_abnormal_return(row['TRANS_DATE'], row['PERMNO'], ticker_ar,\n",
    "                                                         0, 120, 100),\n",
    "        axis=1#, result_type ='expand'\n",
    "    )\n",
    "\n",
    "    return ticker_ar, trans_ticker_price "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create abnormal_ret column for each ticker\n",
    "Steps for each ticker:\n",
    "\n",
    "1. merge transaction data on the same date with the nearest average CRSP stock price (PRC), in order to get most likely PERMNO for the ticker\n",
    "2. Based on number of unique PERMNOs matched, for each (date-disjoint PERMNO),\n",
    "- call `update_trans_ticker_and_create_ar_compare` function to:\n",
    "     1. Create a dataframe with `date` column matching -6 and +2 of the permno's date range (CAN BE MODIFIED BY HYPERPARAMETER)\n",
    "     2. Based on date, get returns, beta and risk_free_rate from beta_daily and risk_free_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers with multiple permnos: {2: {'disjoint': [], 'overlapping_dates': {'less_than_0.2': [], 'more_than_0.2': []}}, 3: {'disjoint': [], 'overlapping_dates': {'less_than_0.2': [], 'more_than_0.2': []}}, 4: {'disjoint': [], 'overlapping_dates': {'less_than_0.2': [], 'more_than_0.2': []}}}\n",
      "Tickers not found in price data: []\n",
      "Tickers not found in transaction data: []\n",
      "Total 3 tickers processed\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "existing_time_frame = {}\n",
    "count = 0\n",
    "count_ticker = 0\n",
    "tickers_multiple_permno = {2: {'disjoint':[], 'overlapping_dates':{'less_than_0.2':[],'more_than_0.2':[]}}, \n",
    "                           3: {'disjoint':[], 'overlapping_dates':{'less_than_0.2':[],'more_than_0.2':[]}}, \n",
    "                           4: {'disjoint':[], 'overlapping_dates':{'less_than_0.2':[],'more_than_0.2':[]}}}\n",
    "ticker_not_found_price = []\n",
    "ticker_not_found_transaction = []\n",
    "\n",
    "# Stores the transactions data, with abnormal return columns\n",
    "all_trans_ar = pd.DataFrame()\n",
    "# Stores the daily stock price data, with abnormal return columns - this is for cumulative abnormal returns\n",
    "all_ticker_ar_compare = pd.DataFrame()\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    count_ticker += 1\n",
    "    try: # just to check if the file exists \n",
    "        pd.read_csv(f\"{RAW_DATA_FOLDER_AR}/crsp/daily_stock_data_by_ticker/{ticker}.csv\",nrows=1)\n",
    "    except FileNotFoundError:\n",
    "        ticker_not_found_price.append(ticker)\n",
    "        continue\n",
    "    else:\n",
    "        stock_price = pd.read_csv(f\"{RAW_DATA_FOLDER_AR}/crsp/daily_stock_data_by_ticker/{ticker}.csv\")\n",
    "        stock_price = stock_price.dropna(subset=['PRC'])\n",
    "        stock_price['date'] = pd.to_datetime(stock_price['date'], errors='coerce')\n",
    "\n",
    "\n",
    "    trans_ticker = all_transactions_final[all_transactions_final['ISSUERTRADINGSYMBOL'] == ticker]\n",
    "\n",
    "    # break if ticker not found in our filtered transaction data\n",
    "    if trans_ticker.shape[0] == 0:\n",
    "        ticker_not_found_transaction.append(ticker)\n",
    "        continue\n",
    "    \n",
    "    # Filter stock price to match transaction date range\n",
    "    start, end = str(min(trans_ticker['TRANS_DATE']))[:10], str(max(trans_ticker['TRANS_DATE']))[:10]\n",
    "    stock_price = stock_price[(stock_price['date'] >= start) & (stock_price['date'] <= end)]\n",
    "\n",
    "    if DEBUG: # commented out\n",
    "        # Check that the date range has a multiple of trading days\n",
    "        # Commented out because it doesn't really matter, as long as there is not alot of NAs (checked for FNCB with rows but 966, expected 2661 days) \n",
    "        '''if (start, end) not in existing_time_frame:\n",
    "            trading_days = len(nyse.schedule(start_date=start, end_date=end))\n",
    "            existing_time_frame[(start, end)] = trading_days\n",
    "        else: trading_days = existing_time_frame[(start, end)]\n",
    "\n",
    "        if stock_price.shape[0] % trading_days != 0:\n",
    "            print(f\"Missing trading days for {ticker} {stock_price.shape[0]}, expected {trading_days}\")\n",
    "        '''\n",
    "        #print(ticker[0], stock_price.shape)\n",
    "        \n",
    "    #### 1. Merge transaction data on the same date with the nearest average CRSP stock price (PRC), in order to get most likely PERMNO for the ticker\n",
    "    # Note that merge_asof is a left join. NA values will be introduced in STOCK_PRICE_COLUMNS if there is no matching date, which happens.\n",
    "    trans_ticker_price = pd.merge_asof(trans_ticker.sort_values('TRANS_PRICEPERSHARE'), stock_price[['PERMNO','date','VOL','PRC','RET','TICKER']].sort_values('PRC'), \n",
    "                                    left_on='TRANS_PRICEPERSHARE', right_on='PRC', left_by='TRANS_DATE', right_by='date', direction='nearest')\n",
    "    \n",
    "    # Apply categorical imputation for PERMNO, because stock_price will have NA values (or missing matching dates) for non-trading days. Whereas those days exist in transaction data\n",
    "    # Impute PERMNO by carrying forward the same value if the before and after values are identical\n",
    "    trans_ticker_price['PERMNO'] = categorical_impute(trans_ticker_price['PERMNO'])\n",
    "    # Consider if we also want to impute volume?\n",
    "\n",
    "    if DEBUG: # Print statement if mismatch rows after merging\n",
    "        if trans_ticker_price.shape[0] != trans_ticker.shape[0]: print(f\"Mismatch in rows after merge for {ticker}: before {trans_ticker.shape[0]}, after {trans_ticker_price.shape[0]}\")\n",
    "    \n",
    "    beta_ticker = all_beta_daily[all_beta_daily['TICKER'] == ticker]\n",
    "    # Check how many unique permcos for each ticker exist in our transaction data\n",
    "    permno_count_dict = trans_ticker_price['PERMNO'].value_counts().to_dict() \n",
    "    permno_unique = len(permno_count_dict)\n",
    "\n",
    "    if permno_unique == 1:\n",
    "        # run function to calculate abnormal returns in the date range for unique permno\n",
    "        permno = list(permno_count_dict.keys())[0]\n",
    "        ticker_ar, trans_ticker_price = update_trans_ticker_and_create_ar_compare(ticker, permno, trans_ticker_price[trans_ticker_price['PERMNO'] == permno], stock_price, beta_ticker)\n",
    "        # Append to the overall dataframe\n",
    "        all_ticker_ar_compare = pd.concat([all_ticker_ar_compare, ticker_ar], axis=0)\n",
    "        all_trans_ar = pd.concat([all_trans_ar, trans_ticker_price], axis=0)\n",
    "\n",
    "    elif permno_unique > 1:\n",
    "        # Check id date ranges for each unqiue permno are disjoint\n",
    "        permno_ranges = trans_ticker_price.groupby('PERMNO')['TRANS_DATE'].agg(['min', 'max']).reset_index()\n",
    "        is_disjoint = True\n",
    "        sorted_ranges = permno_ranges.sort_values(by='min').reset_index(drop=True)\n",
    "        for i in range(len(sorted_ranges) - 1):\n",
    "            if sorted_ranges.loc[i, 'max'] >= sorted_ranges.loc[i + 1, 'min']:\n",
    "                is_disjoint = False\n",
    "                break\n",
    "\n",
    "        if is_disjoint:\n",
    "            # Update this in dictionary to keep track \n",
    "            tickers_multiple_permno[permno_unique]['disjoint'].append(ticker)\n",
    "\n",
    "            # For each unique permno, create abnormal returns comparison and CAR for transaction as per usual\n",
    "            for permno, count in permno_count_dict.items():\n",
    "                # run function to calculate abnormal returns in the date range for each permno\n",
    "                ## Important to have X_new here otherwise will overwrite \n",
    "                ticker_ar_new, trans_ticker_price_new = update_trans_ticker_and_create_ar_compare(ticker, permno, trans_ticker_price[trans_ticker_price['PERMNO'] == permno], stock_price, beta_ticker)\n",
    "                # Append to the overall dataframe\n",
    "                all_ticker_ar_compare = pd.concat([all_ticker_ar_compare, ticker_ar_new], axis=0)\n",
    "                all_trans_ar = pd.concat([all_trans_ar, trans_ticker_price_new], axis=0)\n",
    "        else:\n",
    "            # Count the number of rows per PERMNO\n",
    "            permno_counts = trans_ticker_price['PERMNO'].value_counts()\n",
    "            min_permno, max_permno = permno_counts.idxmin(), permno_counts.idxmax() # get permno of the least and most amount of rows\n",
    "\n",
    "            # Check if the smaller PERMNO is less than 20% of total rows\n",
    "            if permno_counts[min_permno] / len(trans_ticker_price) <= 0.20:\n",
    "                # Update this in dictionary to keep track\n",
    "                tickers_multiple_permno[permno_unique]['overlapping_dates']['less_than_0.2'].append(ticker)\n",
    "\n",
    "                # Not sure if we should assume them as same value..\n",
    "                '''# Replace all occurrences of the lesser PERMNO with the larger one\n",
    "                trans_ticker_price['PERMNO'] = trans_ticker_price['PERMNO'].replace(min_permno, max_permno) ### NOTE THAT HERE THE VOL AND RET RESULTS WOULD THEN BE WRONG...\n",
    "\n",
    "                # Run function as per normal\n",
    "                if permno_unique == 2:\n",
    "                    # run function to calculate abnormal returns in the date range for each permno\n",
    "                    ticker_ar, trans_ticker_price = update_trans_ticker_and_create_ar_compare(ticker, max_permno, trans_ticker_price[trans_ticker_price['PERMNO'] == max_permno], stock_price, beta_ticker)\n",
    "                    # Append to the overall dataframe\n",
    "                    all_ticker_ar_compare = pd.concat([all_ticker_ar_compare, ticker_ar], axis=0)\n",
    "                    all_trans_ar = pd.concat([all_trans_ar, trans_ticker_price], axis=0)'''\n",
    "            \n",
    "            else:\n",
    "                ## Note that overlapping time permno with more than 2 are simply excluded\n",
    "                tickers_multiple_permno[permno_unique]['overlapping_dates']['more_than_0.2'].append(ticker)\n",
    "\n",
    "print(f\"Tickers with multiple permnos: {tickers_multiple_permno}\")\n",
    "print(f\"Tickers not found in price data: {ticker_not_found_price}\")\n",
    "print(f\"Tickers not found in transaction data: {ticker_not_found_transaction}\")\n",
    "print(f\"Total {count_ticker} tickers processed\")\n",
    "## 300 cases took 7 minutes for Emily..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple checking of data before downloading to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1338.000000\n",
       "mean       -0.007387\n",
       "std         0.204393\n",
       "min        -1.717498\n",
       "25%        -0.012362\n",
       "50%        -0.005021\n",
       "75%        -0.000599\n",
       "max         0.663414\n",
       "Name: abnormal_ret, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trans_ar['abnormal_ret'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1328.000000\n",
       "mean       -0.641955\n",
       "std        25.072193\n",
       "min      -197.676295\n",
       "25%        -1.116769\n",
       "50%        -0.747213\n",
       "75%        -0.237183\n",
       "max        70.347152\n",
       "Name: CAR_120_before, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trans_ar['CAR_120_before'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trans_ar['CAR_5_after'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 44)\n",
      "(8693, 14)\n"
     ]
    }
   ],
   "source": [
    "print(all_trans_ar.shape)\n",
    "print(all_ticker_ar_compare.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>RET</th>\n",
       "      <th>RETX</th>\n",
       "      <th>VOL</th>\n",
       "      <th>DATE</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>ret_beta</th>\n",
       "      <th>alpha</th>\n",
       "      <th>b_mkt</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>actual_ret</th>\n",
       "      <th>expected_ret</th>\n",
       "      <th>abnormal_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>OMCL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>89110.0</td>\n",
       "      <td>0.3110%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00311</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>-0.020202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-13</td>\n",
       "      <td>OMCL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>89110.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7886</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>0.021755</td>\n",
       "      <td>-0.020975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date TICKER  RET RETX  VOL       DATE   PERMNO ret_beta  alpha  \\\n",
       "0 2010-02-12   OMCL  NaN  NaN  NaN 2010-02-12  89110.0  0.3110%    0.0   \n",
       "1 2010-02-13   OMCL  NaN  NaN  NaN        NaT  89110.0      NaN    NaN   \n",
       "\n",
       "    b_mkt  risk_free_rate  actual_ret  expected_ret  abnormal_ret  \n",
       "0  0.7915             0.1     0.00311      0.023312     -0.020202  \n",
       "1  0.7886             0.1     0.00078      0.021755     -0.020975  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ticker_ar_compare.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To download data to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All transactions with abnormal return (and CAR)\n",
    "all_trans_ar.to_csv(f\"{PROCESSED_DATA_FOLDER}/transactions_abnormal_returns_{count_ticker}_ticker.csv\")\n",
    "\n",
    "## All tickers (unique PERMNO) with abnormal returns for the benchmark calculations \n",
    "## This dataset is independent of transactions, and should be used whenc calculating abnormal returns. Save to csv for future refence.\n",
    "all_ticker_ar_compare.to_csv(f\"{RAW_DATA_FOLDER_AR}/transactions_abnormal_returns_{count_ticker}_ticker_COMPARE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.095842877375001"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ticker_ar_compare[(all_ticker_ar_compare['TICKER']=='OMCL') & (all_ticker_ar_compare['date']<='2018-03-08')& (all_ticker_ar_compare['date']>='2018-02-28')]['abnormal_ret'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TRANS_SK</th>\n",
       "      <th>ACCESSION_NUMBER</th>\n",
       "      <th>SECURITY_TITLE</th>\n",
       "      <th>TRANS_DATE</th>\n",
       "      <th>DEEMED_EXECUTION_DATE</th>\n",
       "      <th>TRANS_CODE</th>\n",
       "      <th>EQUITY_SWAP_INVOLVED</th>\n",
       "      <th>TRANS_TIMELINESS</th>\n",
       "      <th>TRANS_SHARES</th>\n",
       "      <th>TRANS_PRICEPERSHARE</th>\n",
       "      <th>TRANS_ACQUIRED_DISP_CD</th>\n",
       "      <th>SHRS_OWND_FOLWNG_TRANS</th>\n",
       "      <th>DIRECT_INDIRECT_OWNERSHIP</th>\n",
       "      <th>NATURE_OF_OWNERSHIP</th>\n",
       "      <th>trans_amt</th>\n",
       "      <th>FILING_DATE</th>\n",
       "      <th>PERIOD_OF_REPORT</th>\n",
       "      <th>ISSUERCIK</th>\n",
       "      <th>ISSUERNAME</th>\n",
       "      <th>ISSUERTRADINGSYMBOL</th>\n",
       "      <th>RPTOWNERCIK</th>\n",
       "      <th>RPTOWNERNAME</th>\n",
       "      <th>RPTOWNER_RELATIONSHIP</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>date_x</th>\n",
       "      <th>VOL</th>\n",
       "      <th>PRC</th>\n",
       "      <th>RET</th>\n",
       "      <th>TICKER</th>\n",
       "      <th>date_y</th>\n",
       "      <th>actual_ret</th>\n",
       "      <th>b_mkt</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>expected_ret</th>\n",
       "      <th>abnormal_ret</th>\n",
       "      <th>CAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>1684062</td>\n",
       "      <td>1002422</td>\n",
       "      <td>0001179110-18-003804</td>\n",
       "      <td>Common Stock</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8186.0</td>\n",
       "      <td>44.21</td>\n",
       "      <td>D</td>\n",
       "      <td>211323.0</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>361903.06</td>\n",
       "      <td>2018-03-07</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>926326</td>\n",
       "      <td>OMNICELL, Inc</td>\n",
       "      <td>OMCL</td>\n",
       "      <td>1227556</td>\n",
       "      <td>LIPPS RANDALL A</td>\n",
       "      <td>Director,Officer</td>\n",
       "      <td>89110.0</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>330136.0</td>\n",
       "      <td>44.75</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>OMCL</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>1.3569</td>\n",
       "      <td>1.65</td>\n",
       "      <td>-0.562613</td>\n",
       "      <td>0.581975</td>\n",
       "      <td>5.095843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  TRANS_SK      ACCESSION_NUMBER SECURITY_TITLE TRANS_DATE  \\\n",
       "973     1684062   1002422  0001179110-18-003804   Common Stock 2018-03-06   \n",
       "\n",
       "    DEEMED_EXECUTION_DATE TRANS_CODE EQUITY_SWAP_INVOLVED TRANS_TIMELINESS  \\\n",
       "973                   NaN          S                    0              NaN   \n",
       "\n",
       "     TRANS_SHARES  TRANS_PRICEPERSHARE TRANS_ACQUIRED_DISP_CD  \\\n",
       "973        8186.0                44.21                      D   \n",
       "\n",
       "     SHRS_OWND_FOLWNG_TRANS DIRECT_INDIRECT_OWNERSHIP NATURE_OF_OWNERSHIP  \\\n",
       "973                211323.0                         D                 NaN   \n",
       "\n",
       "     trans_amt FILING_DATE PERIOD_OF_REPORT  ISSUERCIK     ISSUERNAME  \\\n",
       "973  361903.06  2018-03-07       2018-03-06     926326  OMNICELL, Inc   \n",
       "\n",
       "    ISSUERTRADINGSYMBOL  RPTOWNERCIK     RPTOWNERNAME RPTOWNER_RELATIONSHIP  \\\n",
       "973                OMCL      1227556  LIPPS RANDALL A      Director,Officer   \n",
       "\n",
       "      PERMNO     date_x       VOL    PRC       RET TICKER     date_y  \\\n",
       "973  89110.0 2018-03-06  330136.0  44.75  0.019362   OMCL 2018-03-06   \n",
       "\n",
       "     actual_ret   b_mkt  risk_free_rate  expected_ret  abnormal_ret       CAR  \n",
       "973    0.019362  1.3569            1.65     -0.562613      0.581975  5.095843  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trans_ar[(all_trans_ar['TICKER']=='OMCL') & (all_trans_ar['TRANS_DATE']=='2018-03-06')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
